{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] {test1,test2,train,evaluate,shell} ...\n",
      "ipykernel_launcher.py: error: invalid choice: 'C:\\\\Users\\\\MLaogo\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-9a163f23-5b33-49c5-a580-4043a2b38ca7.json' (choose from 'test1', 'test2', 'train', 'evaluate', 'shell')\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2870: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# %load q1_window_github.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Q1: A window into NER\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from util import print_sentence, write_conll\n",
    "from data_util import load_and_preprocess_data, load_embeddings, read_conll, ModelHelper\n",
    "from ner_model import NERModel\n",
    "from defs import LBLS\n",
    "\n",
    "# from report import Report\n",
    "\n",
    "logger = logging.getLogger(\"hw3.q1\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.DEBUG)\n",
    "\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Holds model hyperparams and data information.\n",
    "\n",
    "    The config class is used to store various hyperparameters and dataset\n",
    "    information parameters. Model objects are passed a Config() object at\n",
    "    instantiation.\n",
    "\n",
    "    TODO: Fill in what n_window_features should be, using n_word_features and window_size.\n",
    "    \"\"\"\n",
    "    n_word_features = 2  # Number of features for every word in the input.\n",
    "    window_size = 1  # The size of the window to use.\n",
    "    ### YOUR CODE HERE\n",
    "    n_window_features = (2 * window_size + 1) * n_word_features  # The total number of features used for each window.\n",
    "    ### END YOUR CODE\n",
    "    n_classes = 5\n",
    "    dropout = 0.5\n",
    "    embed_size = 50\n",
    "    hidden_size = 200\n",
    "    batch_size = 2048\n",
    "    n_epochs = 10\n",
    "    lr = 0.001\n",
    "\n",
    "    def __init__(self, output_path=None):\n",
    "        if output_path:\n",
    "            # Where to save things.\n",
    "            self.output_path = output_path\n",
    "        else:\n",
    "            self.output_path = \"results/window/{:%Y%m%d_%H%M%S}/\".format(datetime.now())\n",
    "        self.model_output = self.output_path + \"model.weights\"\n",
    "        self.eval_output = self.output_path + \"results.txt\"\n",
    "        self.log_output = self.output_path + \"log\"\n",
    "        self.conll_output = self.output_path + \"window_predictions.conll\"\n",
    "\n",
    "\n",
    "def make_windowed_data(data, start, end, window_size=1):\n",
    "    \"\"\"Uses the input sequences in @data to construct new windowed data points.\n",
    "\n",
    "    TODO: In the code below, construct a window from each word in the\n",
    "    input sentence by concatenating the words @window_size to the left\n",
    "    and @window_size to the right to the word. Finally, add this new\n",
    "    window data point and its label. to windowed_data.\n",
    "\n",
    "    Args:\n",
    "        data: is a list of (sentence, labels) tuples. @sentence is a list\n",
    "            containing the words in the sentence and @label is a list of\n",
    "            output labels. Each word is itself a list of\n",
    "            @n_features features. For example, the sentence \"Chris\n",
    "            Manning is amazing\" and labels \"PER PER O O\" would become\n",
    "            ([[1,9], [2,9], [3,8], [4,8]], [1, 1, 4, 4]). Here \"Chris\"\n",
    "            the word has been featurized as \"[1, 9]\", and \"[1, 1, 4, 4]\"\n",
    "            is the list of labels.\n",
    "        start: the featurized `start' token to be used for windows at the very\n",
    "            beginning of the sentence.\n",
    "        end: the featurized `end' token to be used for windows at the very\n",
    "            end of the sentence.\n",
    "        window_size: the length of the window to construct.\n",
    "    Returns:\n",
    "        a new list of data points, corresponding to each window in the\n",
    "        sentence. Each data point consists of a list of\n",
    "        @n_window_features features (corresponding to words from the\n",
    "        window) to be used in the sentence and its NER label.\n",
    "        If start=[5,8] and end=[6,8], the above example should return\n",
    "        the list\n",
    "        [([5, 8, 1, 9, 2, 9], 1),\n",
    "         ([1, 9, 2, 9, 3, 8], 1),\n",
    "         ...\n",
    "         ]\n",
    "    \"\"\"\n",
    "\n",
    "    windowed_data = []\n",
    "    for sentence, labels in data:\n",
    "    ### YOUR CODE HERE (5-20 lines)\n",
    "        orig_n = len(sentence)\n",
    "        # extend sentence\n",
    "        sentence = [start] * window_size + sentence + [end] * window_size\n",
    "        l = 0  # index labels\n",
    "        # loop over the original sentence\n",
    "        for i in range(window_size, orig_n + window_size):\n",
    "            temp_feats = []\n",
    "            # loop over the window for feature in original sentence\n",
    "            for j in range(i - window_size, i + window_size + 1):\n",
    "                temp_feats.extend(sentence[j])\n",
    "\n",
    "            # put token features together with label:\n",
    "            temp_f_l = (temp_feats, labels[l])\n",
    "            # put into windowed data:\n",
    "            windowed_data.append(temp_f_l)\n",
    "            # iterate our labels index\n",
    "            l += 1\n",
    "    ### END YOUR CODE\n",
    "    return windowed_data\n",
    "\n",
    "\n",
    "class WindowModel(NERModel):\n",
    "    \"\"\"\n",
    "    Implements a feedforward neural network with an embedding layer and\n",
    "    single hidden layer.\n",
    "    This network will predict what label (e.g. PER) should be given to a\n",
    "    given token (e.g. Manning) by  using a featurized window around the token.\n",
    "    \"\"\"\n",
    "\n",
    "    def add_placeholders(self):\n",
    "        \"\"\"Generates placeholder variables to represent the input tensors\n",
    "\n",
    "        These placeholders are used as inputs by the rest of the model building and will be fed\n",
    "        data during training.  Note that when \"None\" is in a placeholder's shape, it's flexible\n",
    "        (so we can use different batch sizes without rebuilding the model).\n",
    "\n",
    "        Adds following nodes to the computational graph\n",
    "\n",
    "        input_placeholder: Input placeholder tensor of  shape (None, n_window_features), type tf.int32\n",
    "        labels_placeholder: Labels placeholder tensor of shape (None,), type tf.int32\n",
    "        dropout_placeholder: Dropout value placeholder (scalar), type tf.float32\n",
    "\n",
    "        Add these placeholders to self as the instance variables\n",
    "            self.input_placeholder\n",
    "            self.labels_placeholder\n",
    "            self.dropout_placeholder\n",
    "\n",
    "        (Don't change the variable names)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE (~3-5 lines)\n",
    "        self.input_placeholder = tf.placeholder(tf.int32, [None, self.config.n_window_features])\n",
    "        self.labels_placeholder = tf.placeholder(tf.int32, [None,])\n",
    "        self.dropout_placeholder = tf.placeholder(tf.float32)\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def create_feed_dict(self, inputs_batch, labels_batch=None, dropout=1):\n",
    "        \"\"\"Creates the feed_dict for the model.\n",
    "        A feed_dict takes the form of:\n",
    "        feed_dict = {\n",
    "                <placeholder>: <tensor of values to be passed for placeholder>,\n",
    "                ....\n",
    "        }\n",
    "\n",
    "        Hint: The keys for the feed_dict should be a subset of the placeholder\n",
    "                    tensors created in add_placeholders.\n",
    "        Hint: When an argument is None, don't add it to the feed_dict.\n",
    "\n",
    "        Args:\n",
    "            inputs_batch: A batch of input data.\n",
    "            labels_batch: A batch of label data.\n",
    "            dropout: The dropout rate.\n",
    "        Returns:\n",
    "            feed_dict: The feed dictionary mapping from placeholders to values.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE (~5-10 lines)\n",
    "        feed_dict = {self.input_placeholder: inputs_batch, \\\n",
    "                     self.dropout_placeholder: dropout}\n",
    "        if labels_batch is not None:\n",
    "            feed_dict[self.labels_placeholder] = labels_batch\n",
    "        ### END YOUR CODE\n",
    "        return feed_dict\n",
    "\n",
    "    def add_embedding(self):\n",
    "        \"\"\"Adds an embedding layer that maps from input tokens (integers) to vectors and then\n",
    "        concatenates those vectors:\n",
    "            - Creates an embedding tensor and initializes it with self.pretrained_embeddings.\n",
    "            - Uses the input_placeholder to index into the embeddings tensor, resulting in a\n",
    "              tensor of shape (None, n_window_features, embedding_size).\n",
    "            - Concatenates the embeddings by reshaping the embeddings tensor to shape\n",
    "              (None, n_window_features * embedding_size).\n",
    "\n",
    "        Hint: You might find tf.nn.embedding_lookup useful.\n",
    "        Hint: You can use tf.reshape to concatenate the vectors. See following link to understand\n",
    "            what -1 in a shape means.\n",
    "            https://www.tensorflow.org/api_docs/python/array_ops/shapes_and_shaping#reshape.\n",
    "        Returns:\n",
    "            embeddings: tf.Tensor of shape (None, n_window_features*embed_size)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE (!3-5 lines)\n",
    "        embedded = tf.Variable(self.pretrained_embeddings)\n",
    "        embeddings = tf.nn.embedding_lookup(embedded,self.input_placeholder)\n",
    "        embeddings = tf.reshape(embeddings, [-1, self.config.n_window_features * self.config.embed_size])\n",
    "        ### END YOUR CODE\n",
    "        return embeddings\n",
    "\n",
    "    def add_prediction_op(self):\n",
    "        \"\"\"Adds the 1-hidden-layer NN:\n",
    "            h = Relu(xW + b1)\n",
    "            h_drop = Dropout(h, dropout_rate)\n",
    "            pred = h_dropU + b2\n",
    "\n",
    "        Recall that we are not applying a softmax to pred. The softmax will instead be done in\n",
    "        the add_loss_op function, which improves efficiency because we can use\n",
    "        tf.nn.softmax_cross_entropy_with_logits\n",
    "\n",
    "        When creating a new variable, use the tf.get_variable function\n",
    "        because it lets us specify an initializer.\n",
    "\n",
    "        Use tf.contrib.layers.xavier_initializer to initialize matrices.\n",
    "        This is TensorFlow's implementation of the Xavier initialization\n",
    "        trick we used in last assignment.\n",
    "\n",
    "        Note: tf.nn.dropout takes the keep probability (1 - p_drop) as an argument.\n",
    "            The keep probability should be set to the value of dropout_rate.\n",
    "\n",
    "        Returns:\n",
    "            pred: tf.Tensor of shape (batch_size, n_classes)\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.add_embedding()\n",
    "        dropout_rate = self.dropout_placeholder\n",
    "        ### YOUR CODE HERE (~10-20 lines)\n",
    "        b1 = tf.get_variable(name='b1', shape = [self.config.hidden_size,], \\\n",
    "                             initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "        b2 = tf.get_variable(name='b2', shape = [self.config.n_classes], \\\n",
    "                             initializer=tf.contrib.layers.xavier_initializer(seed=2))\n",
    "\n",
    "        W = tf.get_variable(name='W', shape = [self.config.n_window_features * self.config.embed_size, self.config.hidden_size], \\\n",
    "                            initializer=tf.contrib.layers.xavier_initializer(seed=3))\n",
    "\n",
    "        U = tf.get_variable(name='U', shape = [self.config.hidden_size, self.config.n_classes], \\\n",
    "                            initializer=tf.contrib.layers.xavier_initializer(seed=4))\n",
    "\n",
    "        z1 = tf.matmul(x,W) + b1\n",
    "        h = tf.nn.relu(z1)\n",
    "        h_drop = tf.nn.dropout(h, dropout_rate)\n",
    "        pred = tf.matmul(h_drop,U) + b2\n",
    "        ### END YOUR CODE\n",
    "        return pred\n",
    "\n",
    "    def add_loss_op(self, pred):\n",
    "        \"\"\"Adds Ops for the loss function to the computational graph.\n",
    "        In this case we are using cross entropy loss.\n",
    "        The loss should be averaged over all examples in the current minibatch.\n",
    "\n",
    "        Remember that you can use tf.nn.sparse_softmax_cross_entropy_with_logits to simplify your\n",
    "        implementation. You might find tf.reduce_mean useful.\n",
    "        Args:\n",
    "            pred: A tensor of shape (batch_size, n_classes) containing the output of the neural\n",
    "                  network before the softmax layer.\n",
    "        Returns:\n",
    "            loss: A 0-d tensor (scalar)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE (~2-5 lines)\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=pred, labels=self.labels_placeholder)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        ### END YOUR CODE\n",
    "        return loss\n",
    "\n",
    "    def add_training_op(self, loss):\n",
    "        \"\"\"Sets up the training Ops.\n",
    "\n",
    "        Creates an optimizer and applies the gradients to all trainable variables.\n",
    "        The Op returned by this function is what must be passed to the\n",
    "        `sess.run()` call to cause the model to train. See\n",
    "\n",
    "        https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#Optimizer\n",
    "\n",
    "        for more information.\n",
    "\n",
    "        Use tf.train.AdamOptimizer for this model.\n",
    "        Calling optimizer.minimize() will return a train_op object.\n",
    "\n",
    "        Args:\n",
    "            loss: Loss tensor, from cross_entropy_loss.\n",
    "        Returns:\n",
    "            train_op: The Op for training.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE (~1-2 lines)\n",
    "        adam_optim = tf.train.AdamOptimizer(self.config.lr)\n",
    "        train_op = adam_optim.minimize(loss)\n",
    "        ### END YOUR CODE\n",
    "        return train_op\n",
    "\n",
    "    def preprocess_sequence_data(self, examples):\n",
    "        return make_windowed_data(examples, start=self.helper.START, end=self.helper.END,\n",
    "                                  window_size=self.config.window_size)\n",
    "\n",
    "    def consolidate_predictions(self, examples_raw, examples, preds):\n",
    "        \"\"\"Batch the predictions into groups of sentence length.\n",
    "        \"\"\"\n",
    "        ret = []\n",
    "        # pdb.set_trace()\n",
    "        i = 0\n",
    "        for sentence, labels in examples_raw:\n",
    "            labels_ = preds[i:i + len(sentence)]\n",
    "            i += len(sentence)\n",
    "            ret.append([sentence, labels, labels_])\n",
    "        return ret\n",
    "\n",
    "    def predict_on_batch(self, sess, inputs_batch):\n",
    "        \"\"\"Make predictions for the provided batch of data\n",
    "\n",
    "        Args:\n",
    "            sess: tf.Session()\n",
    "            input_batch: np.ndarray of shape (n_samples, n_features)\n",
    "        Returns:\n",
    "            predictions: np.ndarray of shape (n_samples, n_classes)\n",
    "        \"\"\"\n",
    "        feed = self.create_feed_dict(inputs_batch)\n",
    "        predictions = sess.run(tf.argmax(self.pred, axis=1), feed_dict=feed)\n",
    "        return predictions\n",
    "\n",
    "    def train_on_batch(self, sess, inputs_batch, labels_batch):\n",
    "        feed = self.create_feed_dict(inputs_batch, labels_batch=labels_batch,\n",
    "                                     dropout=self.config.dropout)\n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict=feed)\n",
    "        return loss\n",
    "\n",
    "    def __init__(self, helper, config, pretrained_embeddings, report=None):\n",
    "        super(WindowModel, self).__init__(helper, config, report)\n",
    "        self.pretrained_embeddings = pretrained_embeddings\n",
    "\n",
    "        # Defining placeholders.\n",
    "        self.input_placeholder = None\n",
    "        self.labels_placeholder = None\n",
    "        self.dropout_placeholder = None\n",
    "\n",
    "        self.build()\n",
    "\n",
    "\n",
    "def test_make_windowed_data():\n",
    "    sentences = [[[1, 1], [2, 0], [3, 3]]]\n",
    "    sentence_labels = [[1, 2, 3]]\n",
    "    data = zip(sentences, sentence_labels)\n",
    "    w_data = make_windowed_data(data, start=[5, 0], end=[6, 0], window_size=1)\n",
    "\n",
    "    assert len(w_data) == sum(len(sentence) for sentence in sentences)\n",
    "\n",
    "    assert w_data == [\n",
    "        ([5, 0] + [1, 1] + [2, 0], 1,),\n",
    "        ([1, 1] + [2, 0] + [3, 3], 2,),\n",
    "        ([2, 0] + [3, 3] + [6, 0], 3,),\n",
    "    ]\n",
    "\n",
    "\n",
    "def do_test1(_):\n",
    "    logger.info(\"Testing make_windowed_data\")\n",
    "    test_make_windowed_data()\n",
    "    logger.info(\"Passed!\")\n",
    "\n",
    "\n",
    "def do_test2(args):\n",
    "    logger.info(\"Testing implementation of WindowModel\")\n",
    "    config = Config()\n",
    "    helper, train, dev, train_raw, dev_raw = load_and_preprocess_data(args)\n",
    "    embeddings = load_embeddings(args, helper)\n",
    "    config.embed_size = embeddings.shape[1]\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        logger.info(\"Building model...\", )\n",
    "        start = time.time()\n",
    "        model = WindowModel(helper, config, embeddings)\n",
    "        logger.info(\"took %.2f seconds\", time.time() - start)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = None\n",
    "\n",
    "        with tf.Session() as session:\n",
    "            session.run(init)\n",
    "            model.fit(session, saver, train, dev)\n",
    "\n",
    "    logger.info(\"Model did not crash!\")\n",
    "    logger.info(\"Passed!\")\n",
    "\n",
    "\n",
    "def do_train(args):\n",
    "    # Set up some parameters.\n",
    "    config = Config()\n",
    "    helper, train, dev, train_raw, dev_raw = load_and_preprocess_data(args)\n",
    "    embeddings = load_embeddings(args, helper)\n",
    "    config.embed_size = embeddings.shape[1]\n",
    "    helper.save(config.output_path)\n",
    "\n",
    "    handler = logging.FileHandler(config.log_output)\n",
    "    handler.setLevel(logging.DEBUG)\n",
    "    handler.setFormatter(logging.Formatter('%(asctime)s:%(levelname)s: %(message)s'))\n",
    "    logging.getLogger().addHandler(handler)\n",
    "\n",
    "    report = None  # Report(Config.eval_output)\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        logger.info(\"Building model...\", )\n",
    "        start = time.time()\n",
    "        model = WindowModel(helper, config, embeddings)\n",
    "        logger.info(\"took %.2f seconds\", time.time() - start)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        with tf.Session() as session:\n",
    "            session.run(init)\n",
    "            model.fit(session, saver, train, dev)\n",
    "            if report:\n",
    "                report.log_output(model.output(session, dev_raw))\n",
    "                report.save()\n",
    "            else:\n",
    "                # Save predictions in a text file.\n",
    "                output = model.output(session, dev_raw)\n",
    "                sentences, labels, predictions = zip(*output)\n",
    "                predictions = [[LBLS[l] for l in preds] for preds in predictions]\n",
    "                output = zip(sentences, labels, predictions)\n",
    "\n",
    "                with open(model.config.conll_output, 'w') as f:\n",
    "                    write_conll(f, output)\n",
    "                with open(model.config.eval_output, 'w') as f:\n",
    "                    for sentence, labels, predictions in output:\n",
    "                        print_sentence(f, sentence, labels, predictions)\n",
    "\n",
    "\n",
    "def do_evaluate(args):\n",
    "    config = Config(args.model_path)\n",
    "    helper = ModelHelper.load(args.model_path)\n",
    "    input_data = read_conll(args.data)\n",
    "    embeddings = load_embeddings(args, helper)\n",
    "    config.embed_size = embeddings.shape[1]\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        logger.info(\"Building model...\", )\n",
    "        start = time.time()\n",
    "        model = WindowModel(helper, config, embeddings)\n",
    "\n",
    "        logger.info(\"took %.2f seconds\", time.time() - start)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        with tf.Session() as session:\n",
    "            session.run(init)\n",
    "            saver.restore(session, model.config.model_output)\n",
    "            for sentence, labels, predictions in model.output(session, input_data):\n",
    "                predictions = [LBLS[l] for l in predictions]\n",
    "                print_sentence(args.output, sentence, labels, predictions)\n",
    "\n",
    "\n",
    "def do_shell(args):\n",
    "    config = Config(args.model_path)\n",
    "    helper = ModelHelper.load(args.model_path)\n",
    "    embeddings = load_embeddings(args, helper)\n",
    "    config.embed_size = embeddings.shape[1]\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        logger.info(\"Building model...\", )\n",
    "        start = time.time()\n",
    "        model = WindowModel(helper, config, embeddings)\n",
    "        logger.info(\"took %.2f seconds\", time.time() - start)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        with tf.Session() as session:\n",
    "            session.run(init)\n",
    "            saver.restore(session, model.config.model_output)\n",
    "\n",
    "            print(\"\"\"Welcome!\n",
    "You can use this shell to explore the behavior of your model.\n",
    "Please enter sentences with spaces between tokens, e.g.,\n",
    "input> Germany 's representative to the European Union 's veterinary committee .\n",
    "\"\"\")\n",
    "            while True:\n",
    "                # Create simple REPL\n",
    "                try:\n",
    "                    sentence = raw_input(\"input> \")\n",
    "                    tokens = sentence.strip().split(\" \")\n",
    "                    for sentence, _, predictions in model.output(session, [(tokens, [\"O\"] * len(tokens))]):\n",
    "                        predictions = [LBLS[l] for l in predictions]\n",
    "                        print_sentence(sys.stdout, sentence, [\"\"] * len(tokens), predictions)\n",
    "                except EOFError:\n",
    "                    print(\"Closing session.\")\n",
    "                    break\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Trains and tests an NER model')\n",
    "    subparsers = parser.add_subparsers()\n",
    "\n",
    "    command_parser = subparsers.add_parser('test1', help='')\n",
    "    command_parser.set_defaults(func=do_test1)\n",
    "\n",
    "    command_parser = subparsers.add_parser('test2', help='')\n",
    "    command_parser.add_argument('-dt', '--data-train', type=argparse.FileType('r'), default=\"data/tiny.conll\",\n",
    "                                help=\"Training data\")\n",
    "    command_parser.add_argument('-dd', '--data-dev', type=argparse.FileType('r'), default=\"data/tiny.conll\",\n",
    "                                help=\"Dev data\")\n",
    "    command_parser.add_argument('-v', '--vocab', type=argparse.FileType('r'), default=\"data/vocab.txt\",\n",
    "                                help=\"Path to vocabulary file\")\n",
    "    command_parser.add_argument('-vv', '--vectors', type=argparse.FileType('r'), default=\"data/wordVectors.txt\",\n",
    "                                help=\"Path to word vectors file\")\n",
    "    command_parser.set_defaults(func=do_test2)\n",
    "\n",
    "    command_parser = subparsers.add_parser('train', help='')\n",
    "    command_parser.add_argument('-dt', '--data-train', type=argparse.FileType('r'), default=\"data/train.conll\",\n",
    "                                help=\"Training data\")\n",
    "    command_parser.add_argument('-dd', '--data-dev', type=argparse.FileType('r'), default=\"data/dev.conll\",\n",
    "                                help=\"Dev data\")\n",
    "    command_parser.add_argument('-v', '--vocab', type=argparse.FileType('r'), default=\"data/vocab.txt\",\n",
    "                                help=\"Path to vocabulary file\")\n",
    "    command_parser.add_argument('-vv', '--vectors', type=argparse.FileType('r'), default=\"data/wordVectors.txt\",\n",
    "                                help=\"Path to word vectors file\")\n",
    "    command_parser.set_defaults(func=do_train)\n",
    "\n",
    "    command_parser = subparsers.add_parser('evaluate', help='')\n",
    "    command_parser.add_argument('-d', '--data', type=argparse.FileType('r'), default=\"data/dev.conll\",\n",
    "                                help=\"Training data\")\n",
    "    command_parser.add_argument('-m', '--model-path', help=\"Training data\")\n",
    "    command_parser.add_argument('-v', '--vocab', type=argparse.FileType('r'), default=\"data/vocab.txt\",\n",
    "                                help=\"Path to vocabulary file\")\n",
    "    command_parser.add_argument('-vv', '--vectors', type=argparse.FileType('r'), default=\"data/wordVectors.txt\",\n",
    "                                help=\"Path to word vectors file\")\n",
    "    command_parser.add_argument('-o', '--output', type=argparse.FileType('w'), default=sys.stdout, help=\"Training data\")\n",
    "    command_parser.set_defaults(func=do_evaluate)\n",
    "\n",
    "    command_parser = subparsers.add_parser('shell', help='')\n",
    "    command_parser.add_argument('-m', '--model-path', help=\"Training data\")\n",
    "    command_parser.add_argument('-v', '--vocab', type=argparse.FileType('r'), default=\"data/vocab.txt\",\n",
    "                                help=\"Path to vocabulary file\")\n",
    "    command_parser.add_argument('-vv', '--vectors', type=argparse.FileType('r'), default=\"data/wordVectors.txt\",\n",
    "                                help=\"Path to word vectors file\")\n",
    "    command_parser.set_defaults(func=do_shell)\n",
    "\n",
    "    ARGS = parser.parse_args()\n",
    "    if ARGS.func is None:\n",
    "        parser.print_help()\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        ARGS.func(ARGS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32ma:\\anaconda2\\envs\\tensorflow\\lib\\argparse.py\u001b[0m in \u001b[0;36mparse_known_args\u001b[1;34m(self, args, namespace)\u001b[0m\n\u001b[0;32m   1757\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1758\u001b[1;33m             \u001b[0mnamespace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_known_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1759\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_UNRECOGNIZED_ARGS_ATTR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32ma:\\anaconda2\\envs\\tensorflow\\lib\\argparse.py\u001b[0m in \u001b[0;36m_parse_known_args\u001b[1;34m(self, arg_strings, namespace)\u001b[0m\n\u001b[0;32m   1966\u001b[0m         \u001b[1;31m# consume any positionals following the last Optional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1967\u001b[1;33m         \u001b[0mstop_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconsume_positionals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1968\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32ma:\\anaconda2\\envs\\tensorflow\\lib\\argparse.py\u001b[0m in \u001b[0;36mconsume_positionals\u001b[1;34m(start_index)\u001b[0m\n\u001b[0;32m   1922\u001b[0m                 \u001b[0mstart_index\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0marg_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m                 \u001b[0mtake_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1924\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32ma:\\anaconda2\\envs\\tensorflow\\lib\\argparse.py\u001b[0m in \u001b[0;36mtake_action\u001b[1;34m(action, argument_strings, option_string)\u001b[0m\n\u001b[0;32m   1815\u001b[0m             \u001b[0mseen_actions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1816\u001b[1;33m             \u001b[0margument_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margument_strings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1817\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32ma:\\anaconda2\\envs\\tensorflow\\lib\\argparse.py\u001b[0m in \u001b[0;36m_get_values\u001b[1;34m(self, action, arg_strings)\u001b[0m\n\u001b[0;32m   2266\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marg_strings\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2267\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32ma:\\anaconda2\\envs\\tensorflow\\lib\\argparse.py\u001b[0m in \u001b[0;36m_check_value\u001b[1;34m(self, action, value)\u001b[0m\n\u001b[0;32m   2309\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'invalid choice: %(value)r (choose from %(choices)s)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2310\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mArgumentError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mArgumentError\u001b[0m: invalid choice: 'C:\\\\Users\\\\MLaogo\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-9a163f23-5b33-49c5-a580-4043a2b38ca7.json' (choose from 'test1', 'test2', 'train', 'evaluate', 'shell')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-cdd9cd71f1aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    535\u001b[0m     \u001b[0mcommand_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_defaults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdo_shell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 537\u001b[1;33m     \u001b[0mARGS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    538\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mARGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_help\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32ma:\\anaconda2\\envs\\tensorflow\\lib\\argparse.py\u001b[0m in \u001b[0;36mparse_args\u001b[1;34m(self, args, namespace)\u001b[0m\n\u001b[0;32m   1724\u001b[0m     \u001b[1;31m# =====================================\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1725\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mparse_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1726\u001b[1;33m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_known_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1727\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0margv\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1728\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'unrecognized arguments: %s'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32ma:\\anaconda2\\envs\\tensorflow\\lib\\argparse.py\u001b[0m in \u001b[0;36mparse_known_args\u001b[1;34m(self, args, namespace)\u001b[0m\n\u001b[0;32m   1763\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mArgumentError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1764\u001b[0m             \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1765\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1766\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1767\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_parse_known_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg_strings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32ma:\\anaconda2\\envs\\tensorflow\\lib\\argparse.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, message)\u001b[0m\n\u001b[0;32m   2383\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_usage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2384\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'prog'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'message'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2385\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%(prog)s: error: %(message)s\\n'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32ma:\\anaconda2\\envs\\tensorflow\\lib\\argparse.py\u001b[0m in \u001b[0;36mexit\u001b[1;34m(self, status, message)\u001b[0m\n\u001b[0;32m   2370\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2371\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_print_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2372\u001b[1;33m         \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2373\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2374\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSystemExit\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
